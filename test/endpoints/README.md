# LUX Endpoint Testing Framework

A comprehensive automated endpoint testing framework for the LUX MarkLogic backend API. This tool automatically discovers endpoints, generates detailed specifications, and creates ready-to-run Excel test files populated with comprehensive test data from multiple sources. Tests can be executed sequentially or in parallel, generating detailed reports with response times and status codes.

## Features

- **Automated endpoint discovery**: Creates detailed JSON specifications with parameters and handlers of endpoints found in app.js
- **Automated test generation**: Creates Excel files with ready-to-run test cases populated from multiple data sources
- **Multi-source data integration**: Automatically discovers and uses ALL available test data providers
- **Comprehensive reporting**: JSON, CSV, and HTML reports with detailed metrics
- **Report comparison**: Compare the reports from two runs to identify regressions
- **Authentication**: Optionally authenticate with digest credentials
- **Test organization**: Tag-based test filtering and test suite management
- **Performance monitoring**: Response time tracking and timeout handling
- **Error handling**: Detailed error reporting and negative test cases

## Available NPM Scripts

The framework includes several convenient npm scripts for common tasks:

| Script | Command | Description |
|--------|---------|-------------|
| `npm test` | `node run-tests.js ./configs` | Run all tests with default configuration |
| `npm run test:dev` | `node run-tests.js ./configs ./reports` | Run tests and save reports to reports directory |
| `npm run test:save` | `node run-tests.js ./configs ./reports --save-responses` | Run tests with response body saving enabled |
| `npm run create:tests` | `node create-tests.js` | Generate Excel test files from endpoints specification |
| `npm run create:spec` | `node create-endpoints-spec.js` | Analyze Express.js app and generate endpoints specification |
| `npm run compare` | `node compare-reports.js` | Interactive tool to compare two test reports |
| `npm run install-deps` | `npm install` | Install all required dependencies |

## Command Line Options

The test runner supports several command line options for filtering and customization:

### Basic Usage
```bash
node run-tests.js [configDir] [reportsDir] [options]
```

### Options
- `--save-responses, -r`: Save response bodies to disk
- `--dry-run, -d`: Helpful to see resolved configuration **and available filtering options**
- `--providers, -p <providers>`: Comma-separated list of test data providers to use
- `--endpoints, -e <endpoints>`: Comma-separated list of endpoint types to test
- `--help, -h`: Show help message

### Examples

**Run all tests:**
```bash
node run-tests.js
```

**Preview what tests would be run (dry-run mode):**
```bash
node run-tests.js --dry-run
```

**Run tests with specific providers (using class names):**
```bash
node run-tests.js --providers AdvancedSearchQueriesTestDataProvider
```

**Run tests for specific endpoints:**
```bash
node run-tests.js --endpoints search,auto-complete
```

**Combine provider and endpoint filtering:**
```bash
node run-tests.js --providers csv-provider --endpoints search ./configs ./reports
```

**Run with response body saving:**
```bash
node run-tests.js --save-responses --providers csv
```

## Dry-Run Mode

The `--dry-run` (or `-d`) option allows you to preview what tests would be executed without actually running them. This is useful for:

- **Planning test execution**: See exactly which tests will run before committing to a full test run
- **Debugging configuration**: Verify that filtering options work as expected
- **Estimating execution time**: Get a rough estimate of how long the actual test run would take
- **Validating test parameters**: Review the URLs, methods, and parameters that would be used

### Dry-Run Output

When dry-run mode is enabled, the tool will:
- Show all discovered test configurations
- Display whether each test is enabled or disabled
- Provide execution estimates

**Dry-run example:**
```
$ node run-tests.js --dry-run --endpoints get-search --providers AdvancedSearchQueriesTestDataProvider
Configuration directory: ./configs
Reports directory: ./reports

DRY RUN MODE: Tests will not be executed, only planned test execution will be shown

Loaded 19 endpoint specifications
Test execution directory: reports\test-run-2025-08-22_17-38-11

Resolved:
  Requested providers:
        AdvancedSearchQueriesTestDataProvider
  Requested endpoints:
        get-search

Filtering options:
  Available providers:
        AdvancedSearchQueriesTestDataProvider
        UpdatedAdvancedSearchQueriesTestDataProvider
  Available endpoints:
        delete-data
        get-advanced-search-config
        get-auto-complete
        get-data
        get-facets
        get-health
        get-info
        get-related-list
        get-resolve
        get-search
        get-search-estimate
        get-search-info
        get-search-will-match
        get-stats
        get-tenant-status
        get-translate
        get-version-info
        post-data
        put-data

Found 72 test configurations across 1 files

Test distribution by endpoint type:
  get-search: 72 tests

=== DRY RUN SUMMARY ===
Total tests found: 72
Tests that would be executed: 22
Tests that would be skipped (filtered out or disabled): 50
Estimated execution time: 44s - 220s (rough estimate)

No actual HTTP requests were made.
To execute these tests, run the same command without --dry-run
```

## Workflow Overview

1. **Provide test data**: Implement the TestDataProvider class for each test data source.
2. **Analyze**: Run `npm run create:spec` to discover all endpoints in your Express app
3. **Generate**: Create comprehensive test files with `npm run create:tests` (automatically discovers and uses all available data sources to populate ready-to-run test cases)
4. **Execute**: Run tests with `npm test` or `npm run test:dev` or `npm run test:save` to save the response bodies. Use `--providers` and `--endpoints` options to filter which tests to run.
5. **Report**: Review detailed HTML, CSV, and JSON reports
6. **Compare**: Use `npm run compare` to analyze differences between test runs.  By default, the last two directories in the reports directory are compared.  To override, specify the reports directory, the baseline report file, and the report file to compare to.  Submit the JSON-formatted report files.
7. **Customize** (Optional): Additional test cases may be added within an existing TestDataProvider's implementation data file or directly within a new TestDataProvider's implementation that doesn't use an external data source.  Do not add them to the generated spreadsheets.

## Quick Start

### 1. Install Dependencies

```bash
cd test/endpoints
npm install
```

### 2. Configure Environment Variables

Set up the required environment variables for your target environment:

**Digest Authentication (default):**
```bash
$env:BASE_URL = "http://localhost:8003"
$env:AUTH_TYPE = "digest"
$env:AUTH_USERNAME = "your-username"
$env:AUTH_PASSWORD = "your-password"
```

**For different environments:**
```bash
# Development
$env:BASE_URL = "http://dev-server:8003"
$env:AUTH_TYPE = "digest"
$env:AUTH_USERNAME = "dev-user"
$env:AUTH_PASSWORD = "dev-password"

# Production  
$env:BASE_URL = "https://api.lux.yale.edu"
$env:AUTH_TYPE = "digest"
$env:AUTH_USERNAME = "prod-user"
$env:AUTH_PASSWORD = "secure-prod-password"
```

### 3. Implement Test Data Providers

_You can double back to this section if you would first like to see the output when only using the same test data providers._

Implement the TestDataProvider class for each data source to generate tests from.

To add a new data source, simply:

1. Create a new provider class extending TestDataProvider
2. Ensure the class name is unique from all other providers (duplicate names interfere with filtering)
3. Implement extractTestData()
4. Register it in [./test-data-providers/index.js](./test-data-providers/index.js)

For example, to add a JSON file provider:

```javascript
export class JsonTestDataProvider extends TestDataProvider {
  constructor(options = {}) {
    super(options);
    this.sourcePath = path.resolve(__dirname, '../test-data/tests.json');
  }
  
  async extractTestData(apiDef, endpointKey, columns) {
    // Implementation specific to JSON file
  }
}
```

### 4. Generate Endpoint Specification and Tests

First, generate the endpoint specification by analyzing the app.js file:

```bash
npm run create:spec
```

This creates an `endpoints-spec.json` file containing all discovered endpoints with their parameters, required fields, and handler information.

Then, create Excel test files based on the specification:

```bash
npm run create:tests
```

This creates comprehensive Excel test files for each endpoint in the `configs/` directory based on the endpoints found in `endpoints-spec.json`:
- Individual Excel files for each discovered endpoint with ready-to-run test cases
- Parameter-specific columns for each endpoint type  
- Pre-configured test data from all available data sources
- Multiple test scenarios including success and error cases
- No manual configuration required - files are ready to execute immediately

### 5. Run Tests

**Using npm scripts (Recommended):**
```bash
npm test
```

**With response body saving:**
```bash
npm run test:save
```

**Using Node.js directly:**
```bash
node run-tests.js ./configs
```

**With custom output directory:**
```bash
node run-tests.js ./configs ./reports
```

**With response body saving:**
```bash
node run-tests.js ./configs ./reports --save-responses
```

**With provider filtering (using class names):**
```bash
node run-tests.js --providers AdvancedSearchQueriesTestDataProvider
```

**With endpoint filtering (run tests for specific endpoint types):**
```bash
node run-tests.js --endpoints search,auto-complete,facets
```

**With combined filtering:**
```bash
node run-tests.js --providers csv-provider --endpoints search ./configs ./reports
```

**Preview filtered tests (dry-run with filtering):**
```bash
node run-tests.js --dry-run --providers csv-provider --endpoints search
```

### 6. Compare Test Results

After running tests multiple times, you can compare results to identify regressions or changes:

```bash
npm run compare
```

This launches an interactive comparison tool that:
- Shows detailed differences between two test reports
- Identifies regressions (tests that were passing but now fail)
- Highlights performance changes (response time differences)
- Generates comparison reports in JSON, HTML, and console formats

## Endpoint Discovery and Specification Generation

The framework includes automated endpoint discovery that analyzes your Express.js application to generate detailed API specifications.

### Endpoint Analysis Script

The `create-endpoints-spec.js` script automatically analyzes your Express.js application to discover:

- All defined routes and their HTTP methods
- Path parameters and query parameters
- Required vs optional parameters
- Handler function names and middleware
- Response patterns and status codes

**Usage:**
```bash
npm run create:spec
```

**Output:** 
Creates `endpoints-spec.json` with detailed endpoint specifications in the format:
```json
{
  "/api/search/:scope": {
    "path": "/api/search/:scope",
    "method": "GET",
    "parameters": [
      {
        "name": "scope",
        "type": "path",
        "required": true,
        "description": "Search scope (work, person, place, concept)"
      },
      {
        "name": "q",
        "type": "query", 
        "required": false,
        "description": "Search query string"
      }
    ],
    "handlerName": "handleSearch",
    "middleware": ["authenticate", "validateScope"]
  }
}
```

#### Test Data Provider System

The framework includes a simplified test data provider system where each provider is specific to a single data source:

**Provider Interface (`test-data-providers/interface.js`):**
- Abstract `TestDataProvider` base class with simplified methods
- `TestDataProviderFactory` for instantiating all registered providers
- Each provider implementation is specific to one data source

**Available Providers:**

Run `node run-tests.js --dry-run` to see the list of available providers and endpoints.

**Provider System:**
1. Each provider is registered with the `TestDataProviderFactory`
2. All registered providers are instantiated when generating tests
3. Each provider extracts test data for every endpoint
4. Data from all providers is combined into comprehensive test files
5. Provider ID is tracked in each test case for full traceability

**Provider Implementation:**
Each provider implements key methods:
- `getProviderId()`: Returns the class' name for filtering and test traceability
- `extractTestData(apiDef, endpointKey, columns)`: Extracts test data for a specific endpoint
- Graceful error handling - returns empty arrays instead of throwing errors

**Benefits:**
- **Simple**: Each provider handles exactly one data source
- **Traceable**: Every test case includes its provider ID for debugging
- **Robust**: Providers fail gracefully without breaking the system
- **Extensible**: Adding new data sources requires only creating a new provider class
- **Ready-to-run**: Generated files contain actual test cases from all available sources

### Test File Generation with Multi-Source Data Providers

The `create-tests.js` script uses registered test data providers (implementations of the TestDataProvider class) and the generated `endpoints-spec.json` file to create endpoint-specific spreadsheets of tests.

- **Individual files**: Separate Excel file for each discovered endpoint with ready-to-run test cases
- **Parameter columns**: Dedicated columns for each endpoint's specific parameters
- **Required field highlighting**: Visual indicators for required parameters
- **Multi-source data**: Automatically discovers and uses ALL available test data providers
- **Documentation sheets**: Embedded help and parameter descriptions

**Usage:**
```bash
npm run create:tests
# or
node create-tests.js
```

### Shared Utilities

The framework includes shared utility functions in `utils.js` that provide consistent endpoint key generation and parameter handling across all tools. This ensures that endpoint naming and parameter processing is standardized across specification generation, test configuration creation, and test execution.

## Response Body Saving

The framework includes an option to save all HTTP response bodies to disk for detailed analysis, debugging, and archival purposes.

### Enable Response Body Saving

**Using npm scripts:**
```bash
npm run test:save
```

**Using command line flag:**
```bash
node run-tests.js ./configs ./reports --save-responses
```

**Using short flag:**
```bash
node run-tests.js ./configs ./reports -r
```

### Response File Organization

When response body saving is enabled, each test execution creates its own timestamped subdirectory with endpoint-specific subdirectories for response bodies, organized by provider within each endpoint:

```
reports/
â””â”€â”€ test-run-2025-08-20_14-30-15/
    â”œâ”€â”€ endpoint-test-report.json
    â”œâ”€â”€ endpoint-test-report.csv
    â”œâ”€â”€ endpoint-test-report.html
    â””â”€â”€ responses/                   # Only created when --save-responses is used
        â”œâ”€â”€ get-search/              # Subdirectory for get-search.xlsx tests
        â”‚   â”œâ”€â”€ AdvancedSearchQueriesTestDataProvider/
        â”‚   â”‚   â”œâ”€â”€ row_001_basicSearch.json
        â”‚   â”‚   â”œâ”€â”€ row_002_advancedSearch.json
        â”‚   â”‚   â””â”€â”€ ...
        â”‚   â”œâ”€â”€ UpdatedAdvancedSearchQueriesTestDataProvider/
        â”‚   â”‚   â”œâ”€â”€ row_001_updatedSearch.json
        â”‚   â”‚   â””â”€â”€ ...
        â”‚   â””â”€â”€ ...                  # One subdirectory per provider
        â”œâ”€â”€ get-facets/              # Subdirectory for get-facets.xlsx tests
        â”‚   â”œâ”€â”€ AdvancedSearchQueriesTestDataProvider/
        â”‚   â”‚   â”œâ”€â”€ row_001_agentStartDate.json
        â”‚   â”‚   â””â”€â”€ ...
        â”‚   â””â”€â”€ ...                  # One subdirectory per provider
        â”œâ”€â”€ post-document-create/    # Subdirectory for post-document-create.xlsx tests
        â”‚   â”œâ”€â”€ SomeOtherProvider/
        â”‚   â”‚   â”œâ”€â”€ row_001_create.json
        â”‚   â”‚   â””â”€â”€ ...
        â”‚   â””â”€â”€ ...                  # One subdirectory per provider
        â””â”€â”€ ...                      # One subdirectory per test configuration file
â””â”€â”€ test-run-2025-08-20_15-45-20/    # Next test execution
    â”œâ”€â”€ endpoint-test-report.json
    â”œâ”€â”€ endpoint-test-report.csv
    â”œâ”€â”€ endpoint-test-report.html
    â””â”€â”€ responses/
        â”œâ”€â”€ get-search/
        â”‚   â”œâ”€â”€ AdvancedSearchQueriesTestDataProvider/
        â”‚   â””â”€â”€ ...
        â””â”€â”€ ...
```

**Benefits of this structure:**
- **Complete Isolation**: Each test run is fully self-contained
- **Endpoint Organization**: Response files grouped by spreadsheet
- **Easy Navigation**: Quickly find responses for specific test
- **No Conflicts**: No filename conflicts between test runs *when using the same versions of the spreadsheets*\*
- **Selective Storage**: Response bodies only created when explicitly requested
- **Comparison**: In addition to the [report-level comparison feature](#comparison-features), a directory comparison of the response bodies of two runs may be performed
- **Easy Cleanup**: Can delete entire test execution directories or specific endpoint responses

\* We may want to change to test IDs that we do not change once "assigned" to a test.

### Response File Format

Each response file contains complete HTTP response information in JSON format:

```json
{
  "status": 200,
  "statusText": "OK",
  "headers": {
    "content-type": "application/json",
    "content-length": "1234",
    "server": "MarkLogic"
  },
  "data": {
    "results": [...],
    "total": 42
  },
  "config": {
    "url": "http://localhost:8003/ds/lux/search.mjs",
    "method": "GET",
    "headers": {...}
  }
}
```

### Use Cases for Response Body Saving

**Debugging:** Compare actual response data when tests fail
**API Documentation:** Generate examples from real response data
**Data Analysis:** Analyze response patterns and data structures
**Regression Testing:** Compare response bodies between API versions
**Performance Analysis:** Examine response size and structure changes
**Compliance:** Archive API responses for audit purposes

### Integration with Reports

When response body saving is enabled, test reports include a `response_body_file` field that references the saved response file:

**JSON Report:**
```json
{
  "test_name": "search_basic",
  "status": "PASS",
  "response_body_file": "search_basic_2025-08-20_14-30-15.json",
  ...
}
```

**HTML Report:**
The HTML report includes a "Response File" column showing the filename for easy reference.

**CSV Report:**
The CSV includes the response file name for programmatic processing.

## Constants and Type Safety

The testing framework includes a comprehensive definition of constants in [./constants.js](./constants.js) yet its `ENDPOINT_KEYS` may be the only used constant/enum used at this time.  And should the endpoint specification be updated, this constant may need to be updated.

Other constants include:

* HTTP status codes
* Spreadsheet column names
* Search scope names

## Configuration

### Environment Variables

The framework uses the following environment variables:

| Variable | Description | Default | Example |
|----------|-------------|---------|---------|
| `BASE_URL` | Base URL for the MarkLogic API | `http://localhost:8003` | `https://api.lux.yale.edu` |
| `AUTH_TYPE` | Authentication type | `digest` | `digest`, `oauth`, `none` |
| `AUTH_USERNAME` | Username for digest auth | `null` | `luxuser` |
| `AUTH_PASSWORD` | Password for digest auth | `null` | `luxpassword` |

**Setting Environment Variables:**

**PowerShell:**
```bash
$env:BASE_URL = "http://localhost:8003"
$env:AUTH_TYPE = "digest"
$env:AUTH_USERNAME = "your-username"
$env:AUTH_PASSWORD = "your-password"
```

**Command Prompt:**
```cmd
set BASE_URL=http://localhost:8003
set AUTH_TYPE=digest
set AUTH_USERNAME=your-username
set AUTH_PASSWORD=your-password
```

**Linux/macOS:**
```bash
export BASE_URL="http://localhost:8003"
export AUTH_TYPE="digest"
export AUTH_USERNAME="your-username"
export AUTH_PASSWORD="your-password"
```

### Test Configuration Spreadsheet

The endpoint-specific configuration files are automatically generated based on the `endpoints-spec.json` and contain columns tailored to each endpoint's specific parameters.

#### Standard Columns (All Endpoint Types)

| Column | Description | Example | Required |
|--------|-------------|---------|----------|
| `provider_id` | Data source identifier | "AdvancedSearchQueriesTestDataProvider" | Yes |
| `test_name` | Unique test identifier | "Search - Basic Query" | Yes |
| `description` | Test description | "Basic search functionality" | No |
| `enabled` | Whether to run this test | true, false | Yes |
| `expected_status` | Expected HTTP status code | 200, 404, 401 | Yes |
| `timeout_ms` | Request timeout in milliseconds | 10000 | No |
| `max_response_time` | Max acceptable response time | 3000 | No |
| `delay_after_ms` | Delay after test completion | 500 | No |
| `tags` | Comma-separated tags | "search,functional,smoke" | No |

#### Parameter Columns (Dynamically Generated)

Each endpoint gets its own Excel file with parameter columns specific to that endpoint's requirements, automatically discovered from your Express.js application:

**Example for `/api/search/:scope` endpoint:**
- `param:scope` - Search scope (work, person, place, etc.) - **Required**
- `param:q` - Search query string
- `param:page` - Page number  
- `param:pageLength` - Results per page
- `param:sort` - Sort order

**Example for `/api/related-list/:id/:name` endpoint:**
- `param:id` - Record ID - **Required**
- `param:name` - Relationship name - **Required**
- `param:page` - Page number
- `param:pageLength` - Results per page

**Visual Indicators:**
- Required parameters are highlighted in yellow
- Optional parameters use standard formatting
- Each Excel file includes a documentation sheet explaining the endpoint's purpose and parameters

## Authentication Configuration

Authentication is configured globally using environment variables and applies to all tests. The framework supports digest authentication by default, with the ability to configure different authentication types for future OAuth support.

```bash
# Digest authentication (default)
$env:AUTH_TYPE = "digest"
$env:AUTH_USERNAME = "your-username"
$env:AUTH_PASSWORD = "your-password"
```

## Reports

The framework generates three types of reports:

### 1. JSON Report
Detailed machine-readable results with full test data and summary statistics.

### 2. CSV Report  
Tabular format suitable for import into Excel or other analysis tools.

### 3. HTML Report
Human-readable report with:
- Test summary statistics
- Color-coded results (green=pass, red=fail, yellow=error)
- Response time analysis
- Individual test details

## Report Comparison Tool

The framework includes a powerful report comparison tool that helps you identify changes, regressions, and performance differences between test runs.

### Using the Comparison Tool

```bash
npm run compare
```

The tool will prompt you to:
1. Select the first report file (baseline)
2. Select the second report file (comparison target)
3. Choose output formats (JSON, HTML, console)

### Comparison Features

**Regression Detection:**
- Identifies tests that were passing but now fail
- Highlights new failures with detailed error information
- Shows tests that have been fixed (previously failing, now passing)

**Performance Analysis:**
- Compares response times between runs
- Identifies performance regressions (slower responses)
- Shows performance improvements
- Calculates statistical differences in response times

**Status Code Changes:**
- Detects when tests return different status codes
- Identifies unexpected status code changes
- Helps track API behavior changes over time

**New and Missing Tests:**
- Shows tests present in one report but not the other
- Helps track test suite evolution
- Identifies removed or renamed tests

### Sample Comparison Output

**Console Output:**
```
ðŸ“Š COMPARISON SUMMARY
=====================
âœ… Total Tests Compared: 45
ðŸ”´ Regressions Found: 2
ðŸŸ¡ Status Code Changes: 1
âš¡ Performance Changes: 3
ðŸ“ˆ New Tests: 0
ðŸ“‰ Missing Tests: 1

ðŸ”´ REGRESSIONS:
- get-search-basic: Was passing, now failing
  Error: Request timeout after 10000ms
  
- post-document-create: Status changed from 200 to 500
  Error: Internal server error

âš¡ PERFORMANCE CHANGES:
- get-facets: Response time increased from 245ms to 1,200ms (+955ms)
- get-search-advanced: Response time improved from 1,800ms to 650ms (-1,150ms)
```

**HTML Report:**
The tool generates a comprehensive HTML report with:
- Color-coded comparison tables
- Interactive filtering and sorting
- Response time charts and graphs  
- Detailed error information
- Export capabilities for further analysis

**JSON Report:**
Machine-readable format for integration with other tools:
```json
{
  "summary": {
    "totalTests": 45,
    "regressions": 2,
    "statusChanges": 1,
    "performanceChanges": 3,
    "newTests": 0,
    "missingTests": 1
  },
  "regressions": [
    {
      "testName": "get-search-basic",
      "baseline": { "status": "pass", "responseTime": 245 },
      "comparison": { "status": "fail", "error": "Timeout" }
    }
  ],
  "performanceChanges": [...]
}
```

## Extending the Data Provider System

The framework's simplified data provider system makes it easy to add new data sources for test case generation. Each provider is specific to a single data source.

### How to Add New Providers

Adding a new data source is straightforward - each provider implementation is specific to a single source file or data source.

#### Step 1: Create a New Provider Class

Create a new provider class extending `TestDataProvider`. Each provider should be specific to one data source:

```javascript
// test-data-providers/json-provider.js
import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';
import { TestDataProvider } from './interface.js';

/**
 * JSON Test Data Provider
 * 
 * Reads test cases from a specific JSON file: test-data/endpoint-tests.json
 * Each provider implementation should be specific to a single file
 */
export class JsonTestDataProvider extends TestDataProvider {
  /**
   * Constructor
   * @param {Object} options - Options for JSON parsing
   */
  constructor(options = {}) {
    super({
      encoding: 'utf8',
      ...options
    });
    
    // This provider is specific to this JSON file
    this.sourcePath = path.resolve(path.dirname(fileURLToPath(import.meta.url)), '../test-data/endpoint-tests.json');
  }

  /**
   * Parse JSON file and extract test cases
   * @param {Object} apiDef - API definition object
   * @param {string} endpointKey - Unique endpoint key
   * @param {Array<string>} columns - Expected column structure
   * @returns {Promise<Array<Array>>} - Array of test data rows
   */
  async extractTestData(apiDef, endpointKey, columns) {
    try {
      // Check if file exists
      if (!fs.existsSync(this.sourcePath)) {
        console.log(`JSON file not found: ${this.sourcePath} - skipping JSON provider`);
        return [];
      }

      const jsonData = JSON.parse(fs.readFileSync(this.sourcePath, 'utf8'));
      
      // Find tests for this specific endpoint
      const endpointTests = jsonData[endpointKey] || [];
      
      if (endpointTests.length === 0) {
        console.log(`No test data found for ${endpointKey} in JSON file`);
        return [];
      }

      // Convert JSON objects to row arrays matching the column structure
      const testRows = endpointTests.map(testCase => {
        return columns.map(columnName => {
          // Map test case properties to columns
          if (columnName === 'test_name') return testCase.name || '';
          if (columnName === 'description') return testCase.description || '';
          if (columnName === 'enabled') return testCase.enabled !== false ? 'true' : 'false';
          if (columnName === 'expected_status') return testCase.expectedStatus || 200;
          if (columnName === 'timeout_ms') return testCase.timeout || 10000;
          if (columnName === 'max_response_time') return testCase.maxResponseTime || 3000;
          if (columnName === 'delay_after_ms') return testCase.delayAfter || 500;
          if (columnName === 'tags') return testCase.tags || `${endpointKey},json`;
          if (columnName.startsWith('param:')) {
            const paramName = columnName.replace('param:', '');
            return testCase.params ? testCase.params[paramName] || '' : '';
          }
          return ''; // Default for unknown columns
        });
      });

      console.log(`âœ“ Loaded ${testRows.length} test cases from JSON: ${path.basename(this.sourcePath)}`);
      return testRows;

    } catch (error) {
      console.error(`Error reading JSON file ${this.sourcePath}:`, error.message);
      return []; // Return empty array on error to not break other providers
    }
  }
}
```

#### Step 2: Register the New Provider

Add your new provider to `test-data-providers/index.js`:

```javascript
// test-data-providers/index.js
import { TestDataProvider, TestDataProviderFactory } from './interface.js';
import { AdvancedSearchQueriesTestDataProvider } from './csv-provider.js';
import { AdvancedSearchQueriesTestDataProvider } from 
  './google-sheets/search-and-query-tasks-and-test-cases/advanced-search-queries/AdvancedSearchQueriesTestDataProvider.js';

// Register all available providers
TestDataProviderFactory.registerProvider(AdvancedSearchQueriesTestDataProvider);

// ... rest of the file
```

#### Step 3: Create Your Data Source File

Create the data source file that your provider expects:

```javascript
// test-data/endpoint-tests.json
{
  "get-search": [
    {
      "name": "Basic Search Test",
      "description": "Test basic search functionality from JSON",
      "enabled": true,
      "expectedStatus": 200,
      "timeout": 10000,
      "maxResponseTime": 3000,
      "delayAfter": 500,
      "tags": "json,functional,search",
      "params": {
        "q": "test query",
        "scope": "work"
      }
    },
    {
      "name": "Advanced Search Test",
      "description": "Test complex search from JSON",
      "enabled": true,
      "expectedStatus": 200,
      "timeout": 15000,
      "maxResponseTime": 5000,
      "delayAfter": 1000,
      "tags": "json,advanced,search",
      "params": {
        "q": "complex query",
        "scope": "person",
        "page": "1",
        "pageLength": "20"
      }
    }
  ],
  "get-facets": [
    {
      "name": "Facets Test",
      "description": "Test facets endpoint from JSON",
      "enabled": true,
      "expectedStatus": 200,
      "params": {
        "scope": "work"
      }
    }
  ]
}
```

#### Step 4: Test Your Provider

Run the test generation script to verify your provider works:

```bash
npm run create:tests
```

Your new provider will be automatically discovered and used. You'll see output like:

```
$ node create-tests.js
Analyzing endpoints-spec.json to create individual test files...
Reading endpoints specification from: C:\workspaces\yale\clones\lux-middletier\test\endpoints\endpoints-spec.json
Found 19 unique API endpoints

Creating all TestDataProvider instances...
Created 2 provider instance(s):
  - AdvancedSearchQueriesTestDataProvider
  - UpdatedAdvancedSearchQueriesTestDataProvider
...

Test file generation complete!
Next steps:
1. Review generated test cases in the Excel files
2. Add additional test configurations as needed
3. Yellow highlighted columns contain required parameters
4. Run tests with: npm test
```

### Provider Design Principles

- **Single Responsibility**: Each provider is specific to exactly one data source
- **No Configuration**: Providers know their own data source paths internally
- **Graceful Failure**: Return empty arrays instead of throwing errors to avoid breaking other providers
- **Clear Identification**: Provide meaningful provider IDs for test traceability
- **Self-Contained**: Each provider handles its own file format and data structure

### Automated Test Generation vs Manual Customization

The framework now operates with a fully automated approach:

1. **Automated Test Generation** (`create-tests.js`): Automatically creates comprehensive test files with ready-to-run test cases from all available data sources
2. **Manual Customization** (optional): Do not add tests within the generated spreadsheets.  Instead, add to one of the existing TestDataProvider's source files or implement a new TestDataProvider and add to its source file.

The generated Excel files contain complete, ready-to-execute test cases and are overwritten the next time create-tests.js is run (inclusive of `npm run test:*`).

## Advanced Usage

### Running Specific Test Suites

Filter tests by tags:
```javascript
// In your test runner, filter by tags
const testConfigs = this.loadTestConfig()
  .filter(test => test.tags && test.tags.includes('smoke'));
```

### Performance Testing

Configure performance thresholds:
```csv
test_name,max_response_time,timeout_ms,tags
"Performance Test",1000,5000,"performance,load"
```

### Custom Authentication

Support for different authentication methods:
```csv
headers
"Authorization: Bearer jwt-token-here"
"X-API-Key: your-api-key"
"Authorization: Basic base64-encoded-credentials"
```

### Parallel Execution

Tests run sequentially by default for consistent results and easier debugging.

## Integration with Existing Tools

### CI/CD Integration

Example GitHub Actions workflow:
```yaml
- name: Run Endpoint Tests
  run: |
    cd test/endpoints
    npm install
    npm run create:spec
    npm run create:tests  
    npm test
    
- name: Upload Test Reports
  uses: actions/upload-artifact@v3
  with:
    name: endpoint-reports
    path: test/endpoints/reports/
```

## Troubleshooting

### Common Issues

1. **Authentication Failures**: 
   - Verify credentials in environment configuration
   - Check Base64 encoding for Basic auth
   - Ensure service accounts have proper permissions

2. **Timeout Issues**:
   - Increase `timeout_ms` for slow endpoints
   - Check network connectivity
   - Monitor MarkLogic server performance

3. **SSL/TLS Issues**:
   - For local testing, you may need to set `NODE_TLS_REJECT_UNAUTHORIZED=0`
   - Verify certificate configurations

### Debug Mode

Enable verbose logging:
```bash
$env:DEBUG = "true"
npm test
```

Or set environment variable:
```bash
$env:DEBUG = "true"
npm test
```

## Best Practices

1. **Test Organization**: Use tags to group related tests (smoke, functional, performance)
2. **Environment Management**: Maintain separate configurations for each environment
3. **Data Management**: Use realistic test data that doesn't impact production
4. **Error Handling**: Include negative test cases to verify error responses
5. **Performance Monitoring**: Set appropriate response time thresholds
6. **Documentation**: Keep test descriptions up-to-date and meaningful

## Dependencies

- Node.js 16+ 
- axios (HTTP client)
- xlsx (Excel file processing)
- csv-parser (CSV processing)

## Support

For issues or questions:
1. Check the troubleshooting section above
2. Review the generated HTML reports for detailed error information
3. Check MarkLogic server logs for backend issues
4. Verify network connectivity and authentication

## License

This testing framework is part of the LUX project and follows the same licensing terms.
